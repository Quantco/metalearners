{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Feature importance and SHAP values\n",
    "===================================================================\n",
    "\n",
    "Motivation\n",
    "----------\n",
    "\n",
    "As stated in the {ref}`example on LIME explainability <example-lime>`, LIME is a *local*\n",
    "explainability method. This means that LIME focuses on one sample -- or its\n",
    "locality/vicinity/neighborhood -- at a time and tries to imitate the\n",
    "true model behaviour around that sample with a simpler model.\n",
    "\n",
    "On the other hand, SHAP -- short for *SHapley Additive exPlanations* -- is another popular\n",
    "method for interpretability which can provide both local and global interpretations. It\n",
    "uses a cooperative game theory approach to estimate the impact of each feature on the prediction.\n",
    "Local interpretation in SHAP refers to the explanation for a specific prediction made for\n",
    "an individual instance in your dataset. In addition, global interpretation can be achieved\n",
    "in a holistic way by analyzing the importance of each feature across the entire dataset.\n",
    "\n",
    "Lastly, feature importance computation is another widely-used method for global interpretability.\n",
    "While it may not be applicable to all types of models, most tree-based models—including\n",
    "decision trees, random forests, and gradient boosting machines—come with built-in mechanisms\n",
    "for calculating feature importance. This approach ranks the features based on their\n",
    "contributions to the model, providing an overarching view of what features predominantly\n",
    "drive the model's decision-making process.\n",
    "\n",
    "While SHAP and feature importance are typically used in supervised learning scenarios, the key\n",
    "motivation of better understanding a model's behaviour applies just as well to CATE\n",
    "estimation. However, in the context of MetaLearners when compared to the traditional supervised setting,\n",
    "there's a notable challenge. Some MetaLearners, such as the {class}`~metalearners.SLearner`,\n",
    "{class}`~metalearners.TLearner` and {class}`~metalearners.XLearner`, do not possess a direct\n",
    "CATE prediction model. Conversely, others like the {class}`~metalearners.RLearner` and\n",
    "{class}`~metalearners.DRLearner` have a dedicated second stage model which directly estimates the CATE.\n",
    "Given these varied scenarios, we provide examples for both situations in the subsequent sections.\n",
    "Therefore, we illustrate how it can be used with the MetaLearners from ``metalearners``.\n",
    "\n",
    "Background\n",
    "----------\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Most tree-based estimators compute feature importances while fitting, see\n",
    "[here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) and\n",
    "the ``importance_type`` attribute from [LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor)\n",
    "or [XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor).\n",
    "\n",
    "These feature importance measures present valuable insights into the impact of each feature\n",
    "on the CATE estimation.\n",
    "\n",
    "It is important to note that there are different methods of computing the feature importances,\n",
    "for example [LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor)\n",
    "allows to pass ``importance_type=\"split\"`` or ``importance_type=\"gain\"``. In the former\n",
    "feature importances are computed as the number of times the feature is used to split a tree\n",
    "and in the latter the total gain of the splits which use the feature is used. This can affect\n",
    "the results and depending on the use case one may be better than the other.\n",
    "\n",
    "### SHAP values\n",
    "\n",
    "SHAP is a method developed by [Lundberg et al. (2017)](https://www.arxiv.org/abs/1705.07874).\n",
    "SHAP values base their approach on the Shapley value, a concept derived from cooperative\n",
    "game theory. This concept assigns a payout to each player contributing to a cooperative\n",
    "game, depending on their contribution to the total payout. Translated into the realm of\n",
    "machine learning, a *game* is the prediction task, *players* are the features, and the\n",
    "*payout* becomes the prediction.\n",
    "\n",
    "A positive factor of the SHAP values is their desirable properties which come from their\n",
    "theoretical foundation in cooperative game theory. These properties include:\n",
    "\n",
    "* **Local accuracy**: Ensures that the local contribution of the features adds up to the\n",
    "  to the difference of prediction for {math}`x` and the average.\n",
    "* **Missingness**: Ensures that the missing features do not have an impact on the prediction.\n",
    "* **Consistency**: States that if a model changes so that the marginal contribution of\n",
    "  a feature value increases or stays the same (regardless of other features), the Shapley\n",
    "  value also increases or stays the same.\n",
    "\n",
    "Much like the Shapley values that inspired them, computing SHAP values has a high computational\n",
    "cost. Each possible subset of features needs to be considered to calculate exact SHAP\n",
    "values, which leads to an exponential increase in computation with the number of features.\n",
    "However, techniques like Kernel SHAP and Tree SHAP have been developed, which can approximate\n",
    "SHAP values efficiently even over thousands of features.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Just like in our {ref}`example on estimating CATEs with a MetaLearner <example-basic>`,\n",
    "we will first load some experiment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from git_root import git_root\n",
    "df = pd.read_csv(git_root(\"data/learning_mindset.zip\"))\n",
    "outcome_column = \"achievement_score\"\n",
    "treatment_column = \"intervention\"\n",
    "feature_columns = [\n",
    "    column\n",
    "    for column in df.columns\n",
    "    if column not in [outcome_column, treatment_column]\n",
    "]\n",
    "categorical_feature_columns = [\n",
    "    \"ethnicity\",\n",
    "    \"gender\",\n",
    "    \"frst_in_family\",\n",
    "    \"school_urbanicity\",\n",
    "    \"schoolid\",\n",
    "]\n",
    "# Note that explicitly setting the dtype of these features to category\n",
    "# allows both lightgbm as well as shap plots to\n",
    "# 1. Operate on features which are not of type int, bool or float\n",
    "# 2. Correctly interpret categoricals with int values to be\n",
    "#    interpreted as categoricals, as compared to ordinals/numericals.\n",
    "for categorical_feature_column in categorical_feature_columns:\n",
    "    df[categorical_feature_column] = df[categorical_feature_column].astype(\n",
    "        \"category\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the experiment data, we can train a couple of MetaLearners.\n",
    "\n",
    "\n",
    "### Training MetaLearners\n",
    "\n",
    "Again, mirroring our {ref}`example on estimating CATEs with a MetaLearner <example-basic>`,\n",
    "we can train an {class}`~metalearners.rlearner.RLearner` as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from metalearners import RLearner\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "rlearner = RLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"verbose\": -1},\n",
    "    treatment_model_params={\"verbose\": -1},\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    ")\n",
    "rlearner.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also train a {class}`~metalearners.TLearner` to show both scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from metalearners import TLearner\n",
    "tlearner = TLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    ")\n",
    "tlearner.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an {class}`~metalearners.explainer.Explainer` object\n",
    "\n",
    "Now we can need to create an {class}`~metalearners.explainer.Explainer` object which will\n",
    "allow us to compute the feature importance and the SHAP values.\n",
    "\n",
    "This step is the key difference between the models which have a unique treatment model for\n",
    "each variant and the ones that do not.\n",
    "\n",
    "#### The MetaLearner has a unique treatment model for each variant\n",
    "\n",
    "In our trained RLearner the treatment model is a ``LGBMRegressor``, this computes feature\n",
    "importances and can be used for SHAP values calculations, therefore we can directly use\n",
    "it as the model to interpret. We can get an {class}`~metalearners.explainer.Explainer`\n",
    "which uses the final stage models with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rlearner_explainer = rlearner.explainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MetaLearner does not have a unique treatment model for each variant\n",
    "\n",
    "In the scenario where the MetaLearner does not have a unique treatment model for each\n",
    "variant, the proposed solution is to fit an interpretable model for each treatment variant.\n",
    "The input for these models will be the same covariates, while the output is the estimated CATE\n",
    "by the MetaLearner. For this we can use the same method as before\n",
    "{meth}`~metalearners.metalearner.MetaLearner.explainer` but passing also the ``X``,\n",
    "``cate_estimates`` and ``cate_model_factory`` parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tlearner_explainer = tlearner.explainer(\n",
    "    X=df[feature_columns],\n",
    "    cate_estimates=tlearner.predict(X=df[feature_columns], is_oos=False),\n",
    "    cate_model_factory=LGBMRegressor,\n",
    "    cate_model_params={\"verbose\": -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that this method works for every MetaLearner as we can always retrain\n",
    "new models to interpret, for example with the previously trained RLearner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rlearner_explainer_from_estimates = rlearner.explainer(\n",
    "    X=df[feature_columns],\n",
    "    cate_estimates=rlearner.predict(X=df[feature_columns], is_oos=False),\n",
    "    cate_model_factory=LGBMRegressor,\n",
    "    cate_model_params={\"verbose\": -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the feature importance\n",
    "\n",
    "Now we can compute the feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(rlearner_explainer.feature_importances(feature_names=feature_columns, sort_values=True)[0], \"\\n\")\n",
    "print(tlearner_explainer.feature_importances(feature_names=feature_columns, sort_values=True)[0], \"\\n\")\n",
    "print(rlearner_explainer_from_estimates.feature_importances(feature_names=feature_columns, sort_values=True)[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the method {meth}`~metalearners.explainer.Explainer.feature_importances`\n",
    "returns a list of length {math}`n_{variats} -1` that indicates the feature importance for\n",
    "each variant against control. Remember that a higher value means that the corresponding\n",
    "feature is more important for the CATE prediction.\n",
    "\n",
    "### Computing and plotting the SHAP values\n",
    "\n",
    "We can compute the SHAP values with the corresponding calls to {meth}`~metalearners.explainer.Explainer.shap_values`\n",
    "with the desired ``shap_explainer_factory``, in our case as we are are always interpreting\n",
    "``LGBMRegressor`` models we will use [TreeExplainer](https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from shap import TreeExplainer, summary_plot\n",
    "\n",
    "shap_values_rlearner = rlearner_explainer.shap_values(\n",
    "    X=df[feature_columns], shap_explainer_factory=TreeExplainer\n",
    ")\n",
    "summary_plot(shap_values_rlearner[0], features=df[feature_columns])\n",
    "\n",
    "shap_values_tlearner = tlearner_explainer.shap_values(\n",
    "    X=df[feature_columns], shap_explainer_factory=TreeExplainer\n",
    ")\n",
    "summary_plot(shap_values_tlearner[0], features=df[feature_columns])\n",
    "\n",
    "shap_values_rlearner_from_estimates = rlearner_explainer_from_estimates.shap_values(\n",
    "    X=df[feature_columns], shap_explainer_factory=TreeExplainer\n",
    ")\n",
    "summary_plot(shap_values_rlearner_from_estimates[0], features=df[feature_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these SHAP summary plots, the color and orientation of the plotted values help us to understand\n",
    "their impact on model predictions.\n",
    "\n",
    "Each dot in the plot represents a single instance of the given feature present in the data set.\n",
    "The x-axis displays the Shapley value, signifying the strength and directionality of the\n",
    "feature's impact. The y-axis displays a subset of the features in the model.\n",
    "\n",
    "The Shapley value, exhibited on the horizontal axis, is oriented such that values on the\n",
    "right of the center line (0 mark) contribute to a positive shift in the predicted outcome,\n",
    "while those on the left indicate a negative impact.\n",
    "\n",
    "The color coding implemented in these plots is straightforward: red implies a high feature value,\n",
    "while blue denotes a low feature value. This color scheme assists in identifying whether\n",
    "high or low values of a certain feature influence the model's output positively or negatively.\n",
    "The categorical variables are colored in grey.\n",
    "\n",
    "For more guidelines on how to interpret such SHAP plots please see the [SHAP documentation](https://github.com/shap/shap).\n",
    "\n",
    "Note that the method {meth}`~metalearners.explainer.Explainer.shap_values`\n",
    "returns a list of length {math}`n_{variats} -1` that indicates the SHAP values for\n",
    "each variant against control.\n",
    "\n",
    "### Further comments\n",
    "\n",
    "* In the scenario where the model has a unique treatment model for each variant and the used\n",
    "  base model does not compute ``feature_importances_``, the {class}`~metalearners.explainer.Explainer`\n",
    "  object will raise an error when calling {meth}`~metalearners.explainer.Explainer.feature_importances`.\n",
    "  In that case, it is required to retrain a model which computes ``feature_importances_``\n",
    "  by passing ``X``, ``cate_estimates`` and a compatible ``cate_model_factory`` to\n",
    "  {meth}`~metalearners.MetaLearner.explainer`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
