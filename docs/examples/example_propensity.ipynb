{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if I know the propensity score?\n",
    "\n",
    "In some experiment settings we may know beforehand the probabilities of treatment assignments,\n",
    "e.g. if we have data from a {term}`RCT<Randomized Control Trial (RCT)>` with known treatment\n",
    "probabilities.\n",
    "\n",
    "In that case we may not want to learn a propensity model rather just use the known probabilities.\n",
    "\n",
    "Loading the data\n",
    "----------------\n",
    "\n",
    "Just like in our {ref}`example on estimating CATEs with a MetaLearner\n",
    "<example-basic>`, we will first load some experiment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from git_root import git_root\n",
    "\n",
    "df = pd.read_csv(git_root(\"data/learning_mindset.zip\"))\n",
    "outcome_column = \"achievement_score\"\n",
    "treatment_column = \"intervention\"\n",
    "feature_columns = [\n",
    "    column for column in df.columns if column not in [outcome_column, treatment_column]\n",
    "]\n",
    "categorical_feature_columns = [\n",
    "    \"ethnicity\",\n",
    "    \"gender\",\n",
    "    \"frst_in_family\",\n",
    "    \"school_urbanicity\",\n",
    "    \"schoolid\",\n",
    "]\n",
    "# Note that explicitly setting the dtype of these features to category\n",
    "# allows both lightgbm as well as shap plots to\n",
    "# 1. Operate on features which are not of type int, bool or float\n",
    "# 2. Correctly interpret categoricals with int values to be\n",
    "#    interpreted as categoricals, as compared to ordinals/numericals.\n",
    "for categorical_feature_column in categorical_feature_columns:\n",
    "    df[categorical_feature_column] = df[categorical_feature_column].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dummy estimator\n",
    "-----------------------\n",
    "\n",
    "In this tutorial we will assume that we know that all observations were assigned to the\n",
    "treatment with a fixed probability of 0.3, which is close to the fraction of the observations\n",
    "assigned to the treatment group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df[treatment_column].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The fact that we have a fixed propensity score for all observations is not true for this\n",
    "dataset, we just use it for illustrational purposes.\n",
    "```\n",
    "\n",
    "Now we can use a custom ``sklearn``-like classifier: {class}`~metalearners.utils.FixedBinaryPropensity`.\n",
    "The latter can be used like any ``sklearn`` classifier but will always return the same propensity,\n",
    "independently of the observed covariates. This propensity has to be provided at initialization via the\n",
    "``propensity_score`` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the MetaLearner\n",
    "-----------------------\n",
    "\n",
    "Finally we can instantiate and fit our MetaLearner using our own custom propensity model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from metalearners import RLearner\n",
    "from metalearners.utils import FixedBinaryPropensity\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "rlearner = RLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    propensity_model_factory=FixedBinaryPropensity,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"propensity_score\": 0.3},\n",
    "    treatment_model_params={\"verbose\": -1},\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    ")\n",
    "rlearner.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the propensity estimates correspond to our expectation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rlearner.predict_nuisance(\n",
    "    X=df[feature_columns], model_kind=\"propensity_model\", model_ord=0, is_oos=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further comments\n",
    "----------------\n",
    "\n",
    "* This example shows how we can use the same propensity score for all observations in the\n",
    "  binary treatment setting, the class could be easily extended for multiple treatment\n",
    "  variants a. Moreover, customizing the propensity score according to some simple \n",
    "  extracted from the input features could easily be accommodated analogously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
