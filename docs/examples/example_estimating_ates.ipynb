{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Estimating Average Treatment Effects\n",
    "=============================\n",
    "\n",
    "Motivation\n",
    "----------\n",
    "\n",
    "Estimating average treatment effects (ATEs) involves a subset of the tasks involved in estimating Conditional Average Treatment Effects (CATEs), so we can use methods that are designed for estimating CATEs to estimate ATEs. \n",
    "\n",
    "In this example, we simulate some data with a confounded binary treatment and demonstrate the `average_treatment_effect` method of the `DRLearner` class, which estimates the ATE, and compare it to estimates from some other popular libraries (`econML` and `doubleML`). We then show how this approach generalizes to a setting with multiple discrete treatments, and finally to a setting with discrete-valued outcomes.\n",
    "\n",
    "Example\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Treatment with confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_dgp(n, k, pscore_fn, tau_fn, outcome_fn, k_cat=1):\n",
    "    \"\"\"DGP for a confounded treatment assignment dgp\n",
    "\n",
    "    Args:\n",
    "        n (int): sample size\n",
    "        k (int): number of continuous covariates\n",
    "        pscore_fn (lambda): propensity score function\n",
    "        tau_fn (lambda): treatment effect function. Can be scalar for constant effect.\n",
    "        outcome_fn (lambda): outcome DGP\n",
    "    \"\"\"\n",
    "    Sigma = np.random.uniform(-1, 1, (k, k))\n",
    "    Sigma = Sigma @ Sigma.T\n",
    "    Xnum = np.random.multivariate_normal(np.zeros(k), Sigma, n)\n",
    "    # generate categorical variables\n",
    "    Xcat = np.random.binomial(1, 0.5, (n, k_cat))\n",
    "    X = np.c_[Xnum, Xcat]\n",
    "    W = np.random.binomial(1, pscore_fn(X), n)\n",
    "    Y = outcome_fn(X, W, tau_fn)\n",
    "    df = pd.DataFrame(\n",
    "        np.c_[Y, W, X], columns=[\"Y\", \"W\"] + [f\"X{i}\" for i in range(k + 1)]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def outcome_fn(x, w, taufn):\n",
    "    return (\n",
    "        taufn(x) * w\n",
    "        + x[:, 0]\n",
    "        + 2 * x[:, 1] ** 2\n",
    "        + 3 * x[:, 3] * x[:, 1]\n",
    "        + x[:, 2]\n",
    "        + x[:, 3]\n",
    "        + np.random.normal(0, 1, n)\n",
    "    )\n",
    "\n",
    "\n",
    "n, k = 10_000, 3\n",
    "pscore_fn = lambda x: 1 / (1 + np.exp(-x[:, 0] - x[:, 1] - x[:, 2] ** 2 + x[:, 3]))\n",
    "# simulate constant_effects\n",
    "# tau_fn = lambda x: 1 + 2 * x[:, 0] + 3 * x[:, 1] + 4 * x[:, 2] + 5 * x[:, 3]\n",
    "tau_fn = lambda x: 1\n",
    "df = binary_dgp(n, k, pscore_fn, tau_fn=tau_fn, outcome_fn=outcome_fn)\n",
    "outcome_column, treatment_column = \"Y\", \"W\"\n",
    "feature_columns = [f\"X{i}\" for i in range(k + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_lm = smf.ols(f\"{outcome_column} ~ {treatment_column}\", df) .fit(cov_type=\"HC1\")\n",
    "naive_est = naive_lm.params.iloc[1], naive_lm.bse.iloc[1]\n",
    "naive_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaradjust_lm = smf.ols(f\"{outcome_column} ~ {treatment_column}+{'+'.join(feature_columns)}\",\n",
    "                   df) .fit(cov_type=\"HC1\")\n",
    "linreg_est = covaradjust_lm.params.iloc[1], covaradjust_lm.bse.iloc[1]\n",
    "linreg_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model is misspecified, so both the naive and conditional estimates are biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `metalearners`: `DRLearner`\n",
    "\n",
    "Point estimates and standard errors for treatment effects for the AIPW estimator can be computed by aggregating the pseudo-outcome computed by the `DRLearner` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metalearners import DRLearner\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metalearners_dr = DRLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    treatment_model_factory=DummyRegressor, # not actually used since we don't fit treatment model\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"verbose\": -1},\n",
    ")\n",
    "\n",
    "metalearners_dr.fit_all_nuisance(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")\n",
    "metalearners_est = metalearners_dr.average_treatment_effect( # still need to pass data objects since DRLearner does not retain any data\n",
    "    X=df[feature_columns],\n",
    "    w=df[treatment_column],\n",
    "    y=df[outcome_column],\n",
    "    is_oos=False,\n",
    ")\n",
    "metalearners_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual computation with pseudo outcome method produces the same estimate (`average_treatment_effect` does a generalisation of this under the hood) yields the same estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_i = metalearners_dr._pseudo_outcome(\n",
    "    X=df[feature_columns],\n",
    "    w=df[treatment_column],\n",
    "    y=df[outcome_column],\n",
    "    treatment_variant=1,\n",
    "    is_oos=False,\n",
    ")\n",
    "gamma_i.mean(), gamma_i.std()/np.sqrt(n)\n",
    "est, se = gamma_i.mean(), gamma_i.std()/np.sqrt(n)\n",
    "print(f\"est: {est}, se: {se}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `doubleml`: `DoubleMLIRM`\n",
    "\n",
    "The [`doubleML`](https://docs.doubleml.org/stable/index.html) library focuses on estimating average effects and has an 'interactive regression model (IRM)' class that estimates the ATE using the same pseudo-outcome method as the `DRLearner` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLIRM, DoubleMLData\n",
    "dml_data = DoubleMLData(\n",
    "    df,\n",
    "    x_cols=feature_columns,\n",
    "    y_col=outcome_column,\n",
    "    d_cols=treatment_column,\n",
    ")\n",
    "\n",
    "aipw_mod = DoubleMLIRM(\n",
    "    dml_data,\n",
    "    ml_g = LGBMRegressor(),\n",
    "    ml_m = LGBMClassifier(),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "aipw_mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doubleml_est := aipw_mod.summary.values[0, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `econML`: `LinearDRLearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dr import LinearDRLearner\n",
    "import formulaic as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ff := f\"{outcome_column} ~ 0 + {'+'.join(feature_columns)}\")\n",
    "y, X = fm.Formula(ff).get_model_matrix(df, output=\"numpy\")\n",
    "W = df[treatment_column].values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econml_dr = LinearDRLearner(model_regression=LGBMRegressor(), model_propensity=LGBMClassifier())\n",
    "econml_dr.fit(y, T=W, W=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(econml_est := econml_dr.intercept__inference(1).summary_frame().iloc[0, :2].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison\n",
    "\n",
    "All ml-based estimators yield comparable results (both point estimate and standard error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    " np.c_[\n",
    "    naive_est,\n",
    "    linreg_est,\n",
    "    np.hstack(metalearners_est),\n",
    "    doubleml_est,\n",
    "    econml_est,\n",
    "], index = ['est', 'se'],\n",
    "columns = ['naive', 'linreg', 'metalearners', 'doubleml', 'econml']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Treatment Arms\n",
    "\n",
    "Next, we demonstrate the use of the `treatment_effect` method of the `DRLearner` class in a setting with multiple discrete treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def multi_pscore_fn(x, num_treatments=3):\n",
    "    logits = np.zeros((x.shape[0], num_treatments))\n",
    "    logits[:, 0] = -x[:, 0] - x[:, 1] - x[:, 2]  + x[:, 3]\n",
    "    logits[:, 1] = -x[:, 1] - x[:, 2] - x[:, 3]  + x[:, 0]\n",
    "    logits[:, 2] = -x[:, 2] - x[:, 3] - x[:, 0]  + x[:, 1]\n",
    "    # Apply softmax\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    pscores = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    return pscores\n",
    "\n",
    "# And then modify the multi_treatment_dgp function to use these probabilities:\n",
    "\n",
    "def multi_treatment_dgp(n, k, pscore_fn, tau_fn, outcome_fn, k_cat=1, num_treatments=3):\n",
    "    \"\"\"DGP for a confounded multiple treatment assignment\"\"\"\n",
    "    Sigma = np.random.uniform(-1, 1, (k, k))\n",
    "    Sigma = Sigma @ Sigma.T\n",
    "    Xnum = np.random.multivariate_normal(np.zeros(k), Sigma, n)\n",
    "    Xcat = np.random.binomial(1, 0.5, (n, k_cat))\n",
    "    X = np.c_[Xnum, Xcat]\n",
    "\n",
    "    # Generate multiple treatments\n",
    "    pscores = pscore_fn(X, num_treatments)\n",
    "    W = np.zeros((n, num_treatments), dtype=int)\n",
    "    for i in range(n):\n",
    "        W[i, np.random.choice(num_treatments, p=pscores[i])] = 1\n",
    "    Y = outcome_fn(X, W, tau_fn)\n",
    "    df = pd.DataFrame(\n",
    "        np.c_[Y,  X],\n",
    "        columns=[\"Y\"]\n",
    "        + [f\"X{i}\" for i in range(k + 1)],\n",
    "    ).assign(\n",
    "        W=np.argmax(W, axis=1)\n",
    "    )  # convert one-hot encoding to single column\n",
    "    return df\n",
    "\n",
    "\n",
    "def multi_outcome_fn(x, w, taufn):\n",
    "    return (\n",
    "        np.sum(taufn(x) * w, axis=1)  # sum over treatments\n",
    "        + x[:, 0]\n",
    "        + 2 * x[:, 1] ** 2\n",
    "        + 3 * x[:, 3] * x[:, 1]\n",
    "        + x[:, 2]\n",
    "        + x[:, 3]\n",
    "        + np.random.normal(0, 1, n)\n",
    "    )\n",
    "\n",
    "def multi_tau_fn(x):\n",
    "    return np.c_[ 0, 1, 2]\n",
    "\n",
    "n, k = 10_000, 3\n",
    "\n",
    "df_multi = multi_treatment_dgp(\n",
    "    n, k, multi_pscore_fn, tau_fn=multi_tau_fn, outcome_fn=multi_outcome_fn\n",
    ")\n",
    "feature_columns = [f\"X{i}\" for i in range(k + 1)]\n",
    "outcome_column, treatment_column = \"Y\", \"W\"\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_multi = smf.ols(\"Y ~ C(W)\", df_multi).fit()\n",
    "lm_multi.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These estimates are substantially biased, since the model is misspecified (the true outcome and propensity score contains quadratic terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearners_dr_2 = DRLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    treatment_model_factory=DummyRegressor, # not actually used since we don't fit treatment model\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    is_classification=False,\n",
    "    n_variants=3,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"verbose\": -1},\n",
    ")\n",
    "\n",
    "metalearners_dr_2.fit_all_nuisance(\n",
    "    X=df_multi[feature_columns],\n",
    "    y=df_multi[outcome_column],\n",
    "    w=df_multi[treatment_column],\n",
    ")\n",
    "metalearners_est2 = metalearners_dr_2.average_treatment_effect( # still need to pass data objects since DRLearner does not retain any data\n",
    "    X=df_multi[feature_columns],\n",
    "    y=df_multi[outcome_column],\n",
    "    w=df_multi[treatment_column],\n",
    "    is_oos=False,\n",
    ")\n",
    "metalearners_est2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These estimates are less biased than those produced by the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Outcome\n",
    "\n",
    "Finally, we consider a case where the outcome is binary, in which case the DRLearner class is initialized with `is_classification=True` and the nuisance model is a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_dgp(n, k, pscore_fn, tau_fn, outcome_fn, k_cat=1):\n",
    "    \"\"\"DGP for a confounded treatment assignment with binary outcome\"\"\"\n",
    "    Sigma = np.random.uniform(-1, 1, (k, k))\n",
    "    Sigma = Sigma @ Sigma.T\n",
    "    Xnum = np.random.multivariate_normal(np.zeros(k), Sigma, n)\n",
    "    Xcat = np.random.binomial(1, 0.5, (n, k_cat))\n",
    "    X = np.c_[Xnum, Xcat]\n",
    "    W = np.random.binomial(1, pscore_fn(X), n)\n",
    "    Y_prob = outcome_fn(X, W, tau_fn)\n",
    "    Y = np.random.binomial(1, Y_prob).astype(int)\n",
    "    df = pd.DataFrame(\n",
    "        np.c_[Y, W, X], columns=[\"Y\", \"W\"] + [f\"X{i}\" for i in range(k + 1)]\n",
    "    ).assign(Y = pd.to_numeric(Y, downcast='integer'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def classification_outcome_fn(x, w, taufn):\n",
    "    logits = (\n",
    "        taufn(x) * w\n",
    "        + x[:, 0]\n",
    "        + 2 * x[:, 1]\n",
    "        + 3 * x[:, 3] * x[:, 1]\n",
    "        + x[:, 2]\n",
    "        + x[:, 3]\n",
    "    )\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "\n",
    "# Propensity score function\n",
    "pscore_fn = lambda x: 1 / (1 + np.exp(-x[:, 0] - x[:, 1] - x[:, 2] ** 2 + x[:, 3]))\n",
    "\n",
    "# Treatment effect function - constant increase in probability of Y=1\n",
    "classification_tau_fn = lambda x: 0.05\n",
    "\n",
    "n, k = 10_000, 3\n",
    "df_class = classification_dgp(\n",
    "    n, k, pscore_fn, tau_fn=classification_tau_fn, outcome_fn=classification_outcome_fn\n",
    ")\n",
    "feature_columns = [f\"X{i}\" for i in range(k + 1)]\n",
    "outcome_column, treatment_column = \"Y\", \"W\"\n",
    "\n",
    "df_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive TE\n",
    "df_class.groupby(\"W\")[\"Y\"].mean().diff()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearners_dr_3 = DRLearner(\n",
    "    nuisance_model_factory=LGBMClassifier,\n",
    "    treatment_model_factory=DummyRegressor, # not actually used since we don't fit treatment model\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    is_classification=True,\n",
    "    n_variants=2,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"verbose\": -1},\n",
    ")\n",
    "\n",
    "metalearners_dr_3.fit_all_nuisance(\n",
    "    X=df_class[feature_columns],\n",
    "    y=df_class[outcome_column],\n",
    "    w=df_class[treatment_column],\n",
    ")\n",
    "metalearners_est_3 = metalearners_dr_3.average_treatment_effect( # still need to pass data objects since DRLearner does not retain any data\n",
    "    X=df_class[feature_columns],\n",
    "    y=df_class[outcome_column],\n",
    "    w=df_class[treatment_column],\n",
    "    is_oos=False,\n",
    ")\n",
    "metalearners_est_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have lower bias than the naive comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  },
  "mystnb": {
   "execution_timeout": 120
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
