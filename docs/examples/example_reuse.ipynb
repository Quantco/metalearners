{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Reusing base models\n",
    "=============================\n",
    "\n",
    "Motivation\n",
    "----------\n",
    "\n",
    "In our [Why MetaLearners](../../motivation#why-metalearners) section\n",
    "we praise the modularity of MetaLearners. Part of the reason why\n",
    "modularity is useful is because we can actively decouple different\n",
    "parts of the CATE estimation process.\n",
    "\n",
    "Concretely, this decoupling allows for saving lots of compute\n",
    "resources: if we know that we merely want to change *some parts* of a\n",
    "MetaLearner, we may as well reuse the parts that we don't want to\n",
    "change. Enabling this kind of base model reuse was one of the\n",
    "requirements on ``metalearners``, see [Why not causalml or econml](../../motivation#why-not-causalml-or-econml).\n",
    "\n",
    "For instance, imagine trying to tune an R-Learner's - consisting of two\n",
    "nuisance models, a propensity model and an outcome model - propensity\n",
    "model with respect to its R-Loss. In such a scenario we would like to\n",
    "reuse the same outcome model because it isn't affected by the\n",
    "propensity model and thereby save a lot of redundant compute.\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Just like in our [example on estimating CATEs with a MetaLearner](../example_basic/), we will first load some experiment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from git_root import git_root\n",
    "\n",
    "df = pd.read_csv(git_root(\"data/learning_mindset.zip\"))\n",
    "outcome_column = \"achievement_score\"\n",
    "treatment_column = \"intervention\"\n",
    "feature_columns = [\n",
    "    column\n",
    "    for column in df.columns\n",
    "    if column not in [outcome_column, treatment_column]\n",
    "]\n",
    "categorical_feature_columns = [\n",
    "    \"ethnicity\",\n",
    "    \"gender\",\n",
    "    \"frst_in_family\",\n",
    "    \"school_urbanicity\",\n",
    "    \"schoolid\",\n",
    "]\n",
    "# Note that explicitly setting the dtype of these features to category\n",
    "# allows both lightgbm as well as shap plots to\n",
    "# 1. Operate on features which are not of type int, bool or float\n",
    "# 2. Correctly interpret categoricals with int values to be\n",
    "#    interpreted as categoricals, as compared to ordinals/numericals.\n",
    "for categorical_feature_column in categorical_feature_columns:\n",
    "    df[categorical_feature_column] = df[categorical_feature_column].astype(\n",
    "        \"category\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the experiment data, we can train a MetaLearner.\n",
    "\n",
    "\n",
    "### Training a first MetaLearner\n",
    "\n",
    "Again, mirroring our [example on estimating CATEs with a MetaLearner](../example_basic/), we can train an `RLearner` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metalearners.rlearner.RLearner at 0x11470d110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metalearners import RLearner\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "rlearner = RLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    propensity_model_params={\"verbose\": -1},\n",
    "    treatment_model_params={\"verbose\": -1},\n",
    ")\n",
    "\n",
    "rlearner.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By virtue of having fitted the 'overall' MetaLearner, we fitted\n",
    "the base model, too. Thereby we can now reuse some of them if we wish to.\n",
    "\n",
    "### Extracting a basel model from a trained MetaLearner\n",
    "\n",
    "In order to reuse a base model from one MetaLearner for another\n",
    "MetaLearner, we first have to from the former. If, for instance, we\n",
    "are interested in reusing the outcome nuisance model of the\n",
    "`RLearner` we just trained, we can\n",
    "access it via its ``_nuisance_models`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outcome_model': [CrossFitEstimator(n_folds=10, estimator_factory=<class 'lightgbm.sklearn.LGBMRegressor'>, estimator_params={'verbose': -1}, enable_overall=True, random_state=None, _estimators=[LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1)], _estimator_type='regressor', _overall_estimator=LGBMRegressor(verbose=-1), _test_indices=(array([   13,    27,    37, ..., 10364, 10379, 10386]), array([    6,    10,    16, ..., 10373, 10374, 10389]), array([    5,     9,    42, ..., 10363, 10369, 10370]), array([   43,    49,    50, ..., 10366, 10387, 10388]), array([    0,    18,    31, ..., 10361, 10372, 10375]), array([   11,    14,    20, ..., 10345, 10368, 10378]), array([    4,    12,    35, ..., 10377, 10384, 10390]), array([   17,    24,    52, ..., 10350, 10362, 10380]), array([    2,     7,    19, ..., 10376, 10382, 10385]), array([    1,     3,     8, ..., 10365, 10381, 10383])), _n_classes=None, classes_=None)],\n",
       " 'propensity_model': [CrossFitEstimator(n_folds=10, estimator_factory=<class 'lightgbm.sklearn.LGBMClassifier'>, estimator_params={'verbose': -1}, enable_overall=True, random_state=None, _estimators=[LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1), LGBMClassifier(verbose=-1)], _estimator_type='classifier', _overall_estimator=LGBMClassifier(verbose=-1), _test_indices=(array([   13,    27,    37, ..., 10364, 10379, 10386]), array([    6,    10,    16, ..., 10373, 10374, 10389]), array([    5,     9,    42, ..., 10363, 10369, 10370]), array([   43,    49,    50, ..., 10366, 10387, 10388]), array([    0,    18,    31, ..., 10361, 10372, 10375]), array([   11,    14,    20, ..., 10345, 10368, 10378]), array([    4,    12,    35, ..., 10377, 10384, 10390]), array([   17,    24,    52, ..., 10350, 10362, 10380]), array([    2,     7,    19, ..., 10376, 10382, 10385]), array([    1,     3,     8, ..., 10365, 10381, 10383])), _n_classes=2, classes_=array([0, 1]))]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlearner._nuisance_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the `RLearner` has two\n",
    "kinds of nuisance models: ``\"propensity_model\"`` and ``\"outcome_model\"``. Note\n",
    "that we could've figured this out by calling its <a href=\"../../api_documentation/#metalearners.RLearner.nuisance_model_specifications\"><code>nuisance_model_specifications</code></a> method,\n",
    "too.\n",
    "\n",
    "Therefore, we now know how to fetch our outcome model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CrossFitEstimator(n_folds=10, estimator_factory=<class 'lightgbm.sklearn.LGBMRegressor'>, estimator_params={'verbose': -1}, enable_overall=True, random_state=None, _estimators=[LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1), LGBMRegressor(verbose=-1)], _estimator_type='regressor', _overall_estimator=LGBMRegressor(verbose=-1), _test_indices=(array([   13,    27,    37, ..., 10364, 10379, 10386]), array([    6,    10,    16, ..., 10373, 10374, 10389]), array([    5,     9,    42, ..., 10363, 10369, 10370]), array([   43,    49,    50, ..., 10366, 10387, 10388]), array([    0,    18,    31, ..., 10361, 10372, 10375]), array([   11,    14,    20, ..., 10345, 10368, 10378]), array([    4,    12,    35, ..., 10377, 10384, 10390]), array([   17,    24,    52, ..., 10350, 10362, 10380]), array([    2,     7,    19, ..., 10376, 10382, 10385]), array([    1,     3,     8, ..., 10365, 10381, 10383])), _n_classes=None, classes_=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_models = rlearner._nuisance_models[\"outcome_model\"]\n",
    "outcome_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``outcome_models`` is a sequence of models - in this case of length 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a second MetaLearner by reusing a base model\n",
    "\n",
    "Given that we know have an already trained outcome model, we can reuse\n",
    "for another 'kind' of `RLearner` on the\n",
    "same data. Concretely, we will now want to use a different\n",
    "``propensity_model_factory`` and ``treatment_model_factory``. Note that\n",
    "this time, we do not specify a ``nuisance_model_factory`` in the\n",
    "initialization of the `RLearner` since\n",
    "the `RLearner` only relies on a single\n",
    "non-propensity nuisance model. This might vary for other MetaLearners,\n",
    "such as the `DRLearner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metalearners.rlearner.RLearner at 0x1146d41d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "rlearner_new = RLearner(\n",
    "    propensity_model_factory=LogisticRegression,\n",
    "    treatment_model_factory=LinearRegression,\n",
    "    is_classification=False,\n",
    "    fitted_nuisance_models={\"outcome_model\": outcome_models},\n",
    "    propensity_model_params={\"max_iter\": 500},\n",
    "    n_variants=2,\n",
    ")\n",
    "\n",
    "rlearner_new.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "What's more is that we can also reuse models between different kinds\n",
    "of MetaLearner architectures. A propensity model, for instance, is\n",
    "used in many scenarios. Let's reuse it for a `DRLearner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metalearners.drlearner.DRLearner at 0x11471fc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metalearners import DRLearner\n",
    "\n",
    "trained_propensity_model = rlearner._nuisance_models[\"propensity_model\"][0]\n",
    "\n",
    "drlearner = DRLearner(\n",
    "    nuisance_model_factory=LGBMRegressor,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    nuisance_model_params={\"verbose\": -1},\n",
    "    treatment_model_params={\"verbose\": -1},\n",
    "    fitted_propensity_model=trained_propensity_model,\n",
    "    is_classification=False,\n",
    "    n_variants=2,\n",
    ")\n",
    "\n",
    "drlearner.fit(\n",
    "    X=df[feature_columns],\n",
    "    y=df[outcome_column],\n",
    "    w=df[treatment_column],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further comments\n",
    "\n",
    "* Note that the nuisance models are always expected to be of type <a href=\"../../api_documentation/#metalearners.cross_fit_estimator.CrossFitEstimator\"><code>CrossFitEstimator</code></a>. More\n",
    "  precisely, the when extracting or passing a particular model kind,\n",
    "  we pass a list of `CrossFitEstimator` unless it is the propensity model.\n",
    "* In the examples above we reused nuisance models trained as part of a\n",
    "  call to a MetaLearners overall <a href=\"../../api_documentation/#metalearners.metalearner.MetaLearner.fit\"><code>fit</code></a> method. If one wants to train a nuisance model in isolation (i.e. not\n",
    "  through a MetaLearner) to be used in a MetaLearner afterwards, one\n",
    "  should do it by instantiating `CrossFitEstimator`.\n",
    "* Additionally, individual nuisance models can be trained via a\n",
    "  MetaLearner's <a href=\"../../api_documentation/#metalearners.metalearner.MetaLearner.fit_nuisance\"><code>fit_nuisance</code></a>\n",
    "  method.\n",
    "* We strongly recommend only reusing base models if they have been trained on\n",
    "  exactly the same data. If this is not the case, some functionalities\n",
    "  will probably not work as hoped for.\n",
    "* Note that only [`nuisance models`](../../glossary#nuisance-model) can be reused, not [`treatment models`](../../glossary#treatment-effect-model)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
