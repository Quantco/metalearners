{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating CATEs for survival analysis\n",
    "\n",
    "[Survival analysis](https://en.wikipedia.org/wiki/Survival_analysis), which primarily deals with time-to-event data, often manifests itself in crucial areas such as healthcare (e.g., patient survival times), social sciences, and engineering.\n",
    "In the following example, we will demonstrate the use of our `metalearners` library in survival analysis to estimate {term}`CATEs<Conditional Average Treatment Effect (CATE)>`.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "First we will load the data. For this example we will use a dataset from [Rossi et al. - Money, Work and Crime (1980)](https://gwern.net/doc/sociology/1980-rossi-moneyworkandcrime.pdf). The data describes 432 convicts who were released from Maryland state prisons in the 1970s and who were observed for one year after release. Half the released convicts were assigned at random to an experimental treatment in which they were given financial aid; half did not receive aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from git_root import git_root\n",
    "\n",
    "rossi = pd.read_csv(git_root(\"data/rossi.csv\"))\n",
    "rossi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the ``X``, ``y`` and ``w`` variables necessary for training our MetaLearner. In this dataset we have 6 feature columns but we will also add a column ``\"censored\"`` to ``X`` which indicates if the observation was censored (meaning that the event did not occur before a time limit -- one year in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"age\", \"race\", \"wexp\", \"mar\", \"paro\", \"prio\"]\n",
    "X = rossi[feature_columns].copy(deep=True)\n",
    "X[\"censored\"] = 1 - rossi[\"arrest\"]\n",
    "for c in [\"race\", \"wexp\", \"mar\", \"paro\", \"censored\"]:\n",
    "    X[c] = X[c].astype(\"category\")\n",
    "\n",
    "y = rossi[\"week\"]\n",
    "w = rossi[\"fin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the base models\n",
    "\n",
    "Some survival models face two primary challenges:\n",
    "\n",
    "* **Time to event prediction:** Certain models, particularly those relying on the [Cox Proportional Hazards Model](https://en.wikipedia.org/wiki/Proportional_hazards_model), may not directly provide the time to event prediction. Instead, they yield hazard proportions; however, the desired output is the more useful and direct measure of time to the event.\n",
    "\n",
    "* **Censoring Support:** Survival analysis involves dealing with censored data, where the outcome variable ``y`` may not always be a singular real number for each observation and could be an interval instead.\n",
    "\n",
    "\n",
    "To address these issues, we need to construct a model to act as the nuisance model. Here's how we can address the associated problems:\n",
    "\n",
    "* **Time to event prediction:** The principle to remember is that this nuisance model, when invoking the ``predict`` method, must return a time to event prediction for each observation. In this case, we will employ [``xgboost`` with Accelerated Failure Time](https://xgboost.readthedocs.io/en/stable/tutorials/aft_survival_analysis.html), since its output is inherently the time to event. However, other models may necessitate integration over the survival function or require other transformations to get expected survival time predictions.\n",
    "\n",
    "* **Censoring Support:** For handling this issue, we keep ``y`` as is. If the event was observed, the value represents when it occurred, and if not observed, it is the censoring time. We then augment the ``X`` dataset with an extra column (which won't be passed to the model), labeled ``\"censored\"``, indicating whether the observation was censored or not. This column aids in constructing the upper and lower bounds for the output label from the model.\n",
    "\n",
    "\n",
    "We suggest leveraging the base classes from ``sklearn`` to ensure that some essential methods needed for the MetaLearner are pre-defined and can be utilized effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, check_is_fitted\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SurvivalModel(RegressorMixin, BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, booster_params=None) -> None:\n",
    "        self.booster_params = booster_params\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        common_params = {\n",
    "            \"objective\": \"survival:aft\",\n",
    "            \"eval_metric\": \"aft-nloglik\",\n",
    "            \"aft_loss_distribution\": \"normal\",\n",
    "            \"aft_loss_distribution_scale\": 1.20,\n",
    "        }\n",
    "\n",
    "        # X contains \"censored\" column\n",
    "        features = set(X.columns) - {\"censored\"}\n",
    "        dtrain = xgb.DMatrix(X[list(features)], enable_categorical=True)\n",
    "\n",
    "        if (set(np.unique(X[\"censored\"])) - {0, 1}) != set():\n",
    "            raise ValueError(\"censored column should be binary.\")\n",
    "\n",
    "        y_upper_bound = np.where(X[\"censored\"], +np.inf, y)\n",
    "        y_lower_bound = np.array(y)\n",
    "\n",
    "        dtrain.set_float_info(\"label_lower_bound\", y_lower_bound)\n",
    "        dtrain.set_float_info(\"label_upper_bound\", y_upper_bound)\n",
    "\n",
    "        if self.booster_params is None:\n",
    "            booster_params = common_params\n",
    "        else:\n",
    "            booster_params = common_params | self.booster_params\n",
    "\n",
    "        self.bst_ = xgb.train(booster_params, dtrain, num_boost_round=self.n_estimators)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if fit has been called\n",
    "        check_is_fitted(self)\n",
    "        # X contains \"censored\" column\n",
    "        features = set(X.columns) - {\"censored\"}\n",
    "        if (set(np.unique(X[\"censored\"])) - {0, 1}) != set():\n",
    "            raise ValueError(\"censored column should be binary.\")\n",
    "        dtest = xgb.DMatrix(X[list(features)], enable_categorical=True)\n",
    "        return self.bst_.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the censoring weights\n",
    "As suggested in section 3 of [Bang et al. (2000)](https://www.jstor.org/stable/2673467)\n",
    "and in section 3.3.1 of [Xu et al. (2022)](https://arxiv.org/pdf/2207.07758), we need to weight\n",
    "the observations in the treatment models by their censoring weights. We can do this using\n",
    "the following function which estimates them with a Kaplan-Meier estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "def estimate_ipcw_km(outcome, censored, clip=0.95):\n",
    "    \"\"\"\n",
    "    Estimate Inverse Probability of Censoring Weights (IPCW) by nonparametric Kaplan-Meier method\n",
    "    (which is valid in case of noninformative censoring)\n",
    "    \"\"\"\n",
    "    censoring_surv_func = KaplanMeierFitter().fit(outcome, censored)\n",
    "\n",
    "    prob_censoring_weight = censoring_surv_func.survival_function_.loc[\n",
    "        outcome\n",
    "    ].squeeze()\n",
    "\n",
    "    inverse_prob_censoring_weight = 1 / prob_censoring_weight\n",
    "    inverse_prob_censoring_weight = pd.Series(\n",
    "        index=outcome.index, data=inverse_prob_censoring_weight.values\n",
    "    )\n",
    "    # We only want to use complete observations, therefore we give a weight of 0 to censored observations\n",
    "    inverse_prob_censoring_weight[censored.astype(bool)] = 0\n",
    "\n",
    "    # We clip the weights to a percentile to avoid exploding weights\n",
    "    clip_quantile = inverse_prob_censoring_weight.quantile(clip)\n",
    "    inverse_prob_censoring_weight = inverse_prob_censoring_weight.clip(0, clip_quantile)\n",
    "\n",
    "    return inverse_prob_censoring_weight\n",
    "\n",
    "\n",
    "treatment_sample_weight = estimate_ipcw_km(y, X[\"censored\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the MetaLearner\n",
    "\n",
    "Finally we can instantiate any MetaLearner with the ``SurvivalModel`` as the nuisance model and estimate the CATEs.\n",
    "We note that we pass the sample weights only for the {class}`metalearners.DRLearner`, this is because the\n",
    "{class}`metalearners.TLearner` does not have treatment models and the current implementation of the\n",
    "{class}`metalearners.RLearner` does not allow for sample weights, if this feature is desired\n",
    "please open a GitHub issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metalearners import TLearner, DRLearner, RLearner\n",
    "from metalearners.utils import simplify_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tlearner = TLearner(False, 2, SurvivalModel, nuisance_model_params={\"n_estimators\": 5})\n",
    "tlearner.fit(X, y, w)\n",
    "cate_estimates_tlearner = simplify_output(tlearner.predict(X, True))\n",
    "\n",
    "drlearner = DRLearner(\n",
    "    False,\n",
    "    2,\n",
    "    nuisance_model_factory=SurvivalModel,\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    nuisance_model_params={\"n_estimators\": 5},\n",
    "    propensity_model_params={\"verbose\": -1, \"n_estimators\": 5},\n",
    "    treatment_model_params={\"verbose\": -1, \"n_estimators\": 5},\n",
    ")\n",
    "drlearner.fit(\n",
    "    X,\n",
    "    y,\n",
    "    w,\n",
    "    fit_params={\n",
    "        \"treatment\": {\"treatment_model\": {\"sample_weight\": treatment_sample_weight}}\n",
    "    },\n",
    ")\n",
    "cate_estimates_drlearner = simplify_output(drlearner.predict(X, True))\n",
    "\n",
    "rlearner = RLearner(\n",
    "    False,\n",
    "    2,\n",
    "    nuisance_model_factory=SurvivalModel,\n",
    "    propensity_model_factory=LGBMClassifier,\n",
    "    treatment_model_factory=LGBMRegressor,\n",
    "    nuisance_model_params={\"n_estimators\": 5},\n",
    "    propensity_model_params={\"verbose\": -1, \"n_estimators\": 5},\n",
    "    treatment_model_params={\"verbose\": -1, \"n_estimators\": 5},\n",
    ")\n",
    "rlearner.fit(X, y, w)\n",
    "cate_estimates_rlearner = simplify_output(rlearner.predict(X, True))\n",
    "\n",
    "bins = np.histogram(\n",
    "    np.hstack(\n",
    "        (cate_estimates_tlearner, cate_estimates_drlearner, cate_estimates_rlearner)\n",
    "    ),\n",
    "    bins=40,\n",
    ")[1]\n",
    "\n",
    "plt.hist(cate_estimates_tlearner, bins=list(bins), alpha=0.5, label=\"TLearner\")\n",
    "plt.hist(cate_estimates_drlearner, bins=list(bins), alpha=0.5, label=\"DRLearner\")\n",
    "plt.hist(cate_estimates_rlearner, bins=list(bins), alpha=0.5, label=\"RLearner\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"CATE estimate (weeks)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of CATE estimates\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
